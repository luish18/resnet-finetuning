{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com ResNet\n",
    "\n",
    "## Linha de pensamento. planejamento e problemas encontrados\n",
    "\n",
    "### Modelos\n",
    "\n",
    "Para a resolução do problema de classificação, decidi por utilizar arquiterturas de rede já desenvolvidas. Como o modelo deve ser implementado em um sistema embarcado em uma câmera, busquei por redes que apresentam boa performance de classificação, mas também são relativamente pequenas em termo de tamanho de armazenamento. Além disso, a rede deve apresentar boa velocidade.\n",
    "\n",
    "Durante minha pesquisa encontrei dois candidatos particularmente interessantes.\n",
    "\n",
    "- [MobileNet][1]: redes pensadas com aplicações móveis em mente, atingindo acurácias similares a redes maiores com tempo de inferência e tamanho em memória reduzido.\n",
    "- [ResNet][2]: ainda com tamanho em memória e tempo de inferência reduzidos, ainda maiores que MobileNet. No entando, a ResNet costuma apresentar maior acurácia na classificação\n",
    "\n",
    "Escolhi utilziar a ResNet pela acurácia melhor, mas com o código pronto, ele pode facilmente ser modificado para alterar o modelo.\n",
    "\n",
    "Outro motivo para a escolha destes modelos, é a existência de uma implementação quantizada no TorchHub. Redes neuris quantizadas utilizam inteiros (`int8` por exemplo) para representar os pesos da rede, no lugar de floats. Isso é interessante para uso em sistemas embarcados pois reduz o tamanho em memória de rede e acelera acomputação tanto em treino quanto em inferência.\n",
    "\n",
    "Podemos treinar estes modelos para nossa tarefa de três formas:\n",
    "\n",
    "- Treinamento do zero:\n",
    "  - Treinamos a rede completa no nossso problema. Isso pode nos prover com um modelo melhor na nossa taréfa especifica, mas é mais demandante em termos de computação e tempo.\n",
    "- FineTuning:\n",
    "  - Com feature extractor\n",
    "    - Utilizando os pesos da rede pré treinada, podemos considera ela (com a ultima camada removida) como apenas um extrator de características da imagem. Dessa forma, só temos que treinar um ultimo layer denso que terá a tarefa de, com base nas features extraidas, classificar nosso problema. Utilzando esta abordagem reduzimos drasticamente o tempo e custo computacional de treinamento do modelo\n",
    "  - Sem feature extractor\n",
    "    - Podemos também simplesmente reinicializar os pesos do ultima layer e alterar sua saida para nosso número de classes e, então, atualizar todos os pesos da rede utilizando nossos dados. Assim demorarmos mais no treinamento mas, como a rede complete \"recebe\" informações do nosso problema, podemos atingir melhor desempenho do que no caso com feature extractor\n",
    "\n",
    "Neste problema decidi pela abordagem de Fine tuning sem utilização de um feature extractor.\n",
    "\n",
    "### Framework\n",
    "\n",
    "Escolhi utilizar PyTorch por causa da maior flexibilidade que a biblioteca oferece em relação ao tensorflow, além do ótimo apoio a processamento de imagens com `torchvision`.\n",
    "\n",
    "### Tratamento dos dados\n",
    "\n",
    "#### Loading\n",
    "\n",
    "Para o tratamento dos dados utilzei o módulo `torchdata` do PyTorch. Nesse módulo, são implementadas pipelines de dados básicas como embaralhamento, mapping, collate, entre outras. Estas pipelines podem ser compostas para gerar a pipeline completa de dados. \n",
    "Uma vantagem dessa solução, é a capacidade de carregarmos e colocar em cache nossos dados de forma que, com o link do Gdrive (existêm também implementações para buckets da AWS, Gcloud, Azure ou um HTTP reques genérico), posso baixar e descompactar temporariamente os dados. Com o fim do treinamento os dados são deletados.\n",
    "Dessa forma, para datasets maiores, podemos baixar progressivamente os dados e, para evitar um gargalo gerado pelo dowload, os colocamos em um cache para treinamento.\n",
    "\n",
    "#### Desabalanceamento\n",
    "\n",
    "Os dados de treinamento no dataset fornecido estão extremamente desbalanceados. Para isso, podemos implementar algumas soluções:\n",
    "\n",
    "- Under Sampling:\n",
    "  - Podemos reduzir o número de amostras tomadas das classes com mais exemplos, reduzindo a diferença de representatividade entre as classes\n",
    "- Over Sampling:\n",
    "  - Podemos repetir exemplos de classes pouco representadas com algum tipo de augumentação para introduzir exemplos diferentes\n",
    "- Uso de diferentes métricas:\n",
    "  - Em casos que os dados são desbalanceados, métricas como a acurácia podem nos dar resultados ótimos mesmo que o modelo não esteja classificando corretamente os dados pouco representados (essa situação também é chamada de [paradóxo da acurácia][3]). Por isso, devemos utilzar métricas que levem em conta tanto a precisão como o recall(F1 ou F$\\beta$ por exemplo) \n",
    "- Custom Loss Function\n",
    "  - Podemos \"forçar\" nosso modelo a levar em conta os dados desbalanceados alterando a função de perda. Isso pode ser feito, por exemplo, dando um custo maior para erros nas classes sub-representadas.\n",
    "\n",
    "Neste problema vou criar uma nova função de perda e utilizar diferentes métricas com melhor desempenho para taréfas de classificação.\n",
    "\n",
    "## Problemas para a produção\n",
    "\n",
    "### Performance do modelo\n",
    "\n",
    "#### FPS\n",
    "Ao colocar o modelo em produção, imagino que, mesmo com redes reduzidas e quantizadas, o tempo de inferência do modelos será um problema. Por exemplo, se for esperado que o modelo classifique motos em movimento em uma rua, o fps em que o modelo consegue rodar, pois, dependendo da velocidade da moto, podemos pegar a moto em um frame na câmera mas perdermos ele no classificador. \n",
    "\n",
    "Outra situação poderia ser uma interface da vista câmera para o usurário. Queremos que a vista rode com o mesmo FPS da câmera, mas, para isso ser feito ao vivo, o fps da câmera e do modelo teriam que se sincronizar.\n",
    "\n",
    "Para resolver esse problema, podemos criar um buffer das imagens da câmera que será a entrada para o modelo. Isso criaria um atraso da vista da câmera na interface em relação ao mundo real, o que pode ser tolerável dependendo da situação. Em casos latência é importante, podemos classificar frames da câmera em intervalos determinados como intercalados ou de 3 em 3, por exemplo.\n",
    "\n",
    "#### Ambiente de execução\n",
    "\n",
    "Em ambientes embarcados temos recursos limitados, então, executarmos a inferência em Python pode deteriorar a performance e ocupar muito espaço em memória. Para executar nosso modelo em C++, podemos transformá-lo para TorchScript. Dessa forma, o modelo é compilado e otimizado de tal maneira que pode ser executado em qualque ambiente com PyTorch.\n",
    "\n",
    "## Considerações\n",
    "\n",
    "Com mais tempo para resolver o problema, acredito que poderia desenvolver algumas otimizações para o modelos como:\n",
    "\n",
    "- Tuning de hiperparâmetros\n",
    "  - poderiamos melhorar o desempenho do nosso modelo realizando alguma otimização sobre o espaço de hiperparâmetros da rede\n",
    "- Uso de núvem\n",
    "  - apesar de meu notebook ser suficiente para o treinamento do modelo neste caso, ao adiconarmos datasest cada vez maiores ao treinamento, seria necessário utilizar algum serviço de computação em núvem para realizar o treinamento. Por exemplo, o treinamento poderia ser feito em uma instância de Ec2 da AWS que pode obter os dados para o treinamento de um bucket S3. Assim, o tempo gasto em treinamento seria reduzido e teríamos um ambiente mais facilmente escalável, genérico e gerenciável para usar em outros projetos\n",
    "- Testes de redes alternativas/modificação da arquitetura de rede\n",
    "  - com mais tempo, gostaria de realizar mais testes de diferentes arquiteturas neste problema específico, além de fazer modificações/cortes nestas arquiteturas para checar se, para a complexidade deste problema, seria possível obter melhor performance com redes menores.\n",
    "\n",
    "[1]: <https://doi.org/10.48550/arXiv.1704.04861> \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"\n",
    "[2]: <https://doi.org/10.48550/arXiv.1512.03385> \"Deep Residual Learning for Image Recognition\"\n",
    "[3]: <https://en.wikipedia.org/wiki/Accuracy_paradox#:~:text=The%20accuracy%20paradox%20is%20the,too%20crude%20to%20be%20useful.> \"Accuracy Paradox\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# bibliotecas para plots e manipulacao de dados\n",
    "import pandas as pd\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "\n",
    "# models\n",
    "import torch.nn as nn\n",
    "\n",
    "## metrics and reporting\n",
    "import torch.utils.tensorboard as tboard\n",
    "\n",
    "## data processing and metrics\n",
    "import torchdata.dataloader2 as dl2\n",
    "import torchdata.datapipes as pipes\n",
    "import torchdata.datapipes.iter as idp\n",
    "import torchmetrics as tmetrics\n",
    "import torchmetrics.classification as cmetrics\n",
    "\n",
    "## transforms\n",
    "import torchvision.transforms as TF\n",
    "from PIL import Image\n",
    "from torchvision.models.resnet import ResNet18_Weights, resnet18\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_freq(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Obtém as classes e quantidade de imagens em cada uma delas\n",
    "\n",
    "    Args:\n",
    "        path (Path): path para as pastas com as imagens de cada classe\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int]: dicionário com cada uma das classes e sua frequência no dataset\n",
    "    \"\"\"\n",
    "    class_paths = path.glob(\"*/**\")\n",
    "    class_freq: dict[str, int] = {}\n",
    "\n",
    "    for folder in class_paths:\n",
    "        class_len: int = len(list(folder.glob(\"*.jpg\"))) + len(\n",
    "            list(folder.glob(\"*.png\"))\n",
    "        )\n",
    "\n",
    "        class_freq[folder.name] = class_len\n",
    "\n",
    "    classes_df = (\n",
    "        pd.DataFrame.from_dict(class_freq, orient=\"index\")\n",
    "        .reset_index()\n",
    "        .rename({0: \"frequencia\", \"index\": \"classe\"}, axis=1)\n",
    "    )\n",
    "\n",
    "    return classes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = os.getenv(\"URL\")\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "MODELS_PATH = Path(\"../models/model.pt\")\n",
    "LOG_PATH = Path(\"../logs/runs\")\n",
    "\n",
    "INPUT_SHAPE = (3, 224, 244)\n",
    "BATCH_SIZE = 256\n",
    "SEED = np.random.randint(\n",
    "    low=0,\n",
    "    high=int(10e5),\n",
    ")\n",
    "SPLIT = 0.8\n",
    "N_EPOCHS = 1000\n",
    "\n",
    "\n",
    "LR = 1e-2\n",
    "LR_STEP = 5\n",
    "GAMMA = 0.1\n",
    "F_BETA = 0.5\n",
    "\n",
    "classes = get_class_freq(DATA_PATH)\n",
    "\n",
    "N_IMAGENS = classes[\"frequencia\"].sum()\n",
    "N_CLASSES = len(classes[\"classe\"])\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "freq = get_class_freq(DATA_PATH)\n",
    "\n",
    "# mapeamento de cada label para um inteiro\n",
    "label_mapping = {\n",
    "    classe: torch.tensor(valor)\n",
    "    for classe, valor in zip(freq.classe.values, freq.index.values)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/\")\n",
    "\n",
    "freq = get_class_freq(DATA_PATH)\n",
    "sns.barplot(data=freq, x=\"frequencia\", y=\"classe\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesos para cada classe\n",
    "\n",
    "Queremos que classes menos representadas tenham um peso maior na função de custo. Para fazer isso de forma atutomática, podemos usar como peso a entropia de Shannon de cada classe em relação ao total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = freq[\"frequencia\"].sum()\n",
    "freq[\"probs\"] = freq[\"frequencia\"] / total\n",
    "freq[\"negative_logprob\"] = -np.log2(freq[\"probs\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que a distribuição dos dados de treino por classe estão desbalanceados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesos para as classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "Em primeiro lugar, precisamos prepara nossos dados de uma forma que eles sejam facilmente consumíveis pelo nosso modelo. Para isso, vou utilizar as funções de `datapipe` do pacote `torchdata` do PyTorch e, do mesmo pacote, vou utilizar `dataloader2` que será responsável pelo carregamento das imagens em batches para o modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipe\n",
    "\n",
    "Aqui crio a pipeline de transformação e carregamento dos dados. Os dados são carregados diretamente do link do google drive, mantendo o arquivo ainda salvo em disco durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_label(item):\n",
    "    return label_mapping[Path(item).parent.name]\n",
    "\n",
    "\n",
    "def path_func(path):\n",
    "    temp_dir = Path(\"../temp/\")\n",
    "    return str(temp_dir / Path(path).name)\n",
    "\n",
    "\n",
    "def filter_imgs(file: tuple[str, Image.Image]):\n",
    "    return file.endswith((\".jpg\", \".png\"))\n",
    "\n",
    "\n",
    "def image_encoder_to_tensor(example):\n",
    "    return TF.PILToTensor()(Image.open(BytesIO(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_pipe(\n",
    "    url: str,\n",
    "    N_images: int,\n",
    "    seed: int,\n",
    "    split: float,\n",
    "    transform: TF.Compose,\n",
    "    batch_size: int = 32,\n",
    "    cache_size: int = 1024,\n",
    ") -> tuple[idp.IterDataPipe, idp.IterDataPipe]:\n",
    "    IMAGEM_IDX = 1\n",
    "    LABEL_IDX = 0\n",
    "\n",
    "    # link para o arquivo no gdrive\n",
    "    url_wrapper = idp.IterableWrapper([url])\n",
    "\n",
    "    # cache do dataset em disco\n",
    "    disk_cache = (\n",
    "        url_wrapper.on_disk_cache(filepath_fn=path_func)\n",
    "        .read_from_gdrive()\n",
    "        .end_caching(same_filepath_fn=True)\n",
    "    )\n",
    "\n",
    "    data_pipe = (\n",
    "        disk_cache.open_files(mode=\"rb\")\n",
    "        .load_from_zip()\n",
    "        .filter(filter_fn=filter_imgs, input_col=LABEL_IDX)\n",
    "        .map(path_to_label, input_col=LABEL_IDX, output_col=LABEL_IDX)\n",
    "        .read_from_stream()\n",
    "        .map(image_encoder_to_tensor, input_col=IMAGEM_IDX, output_col=IMAGEM_IDX)\n",
    "    )\n",
    "\n",
    "    if transform:\n",
    "        data_pipe = data_pipe.map(\n",
    "            transform, input_col=IMAGEM_IDX, output_col=IMAGEM_IDX\n",
    "        )\n",
    "\n",
    "    data_pipe = (\n",
    "        data_pipe.shuffle()\n",
    "        .in_memory_cache(cache_size)\n",
    "        .prefetch(batch_size)\n",
    "        .batch(batch_size=batch_size, drop_last=True)\n",
    "        .collate()\n",
    "    )\n",
    "\n",
    "    split_dict = {\"train\": split, \"valid\": 1 - split}\n",
    "    train, valid = data_pipe.random_split(\n",
    "        weights=split_dict, total_length=N_images, seed=seed\n",
    "    )\n",
    "\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_pipe(\n",
    "    file: Path | str,\n",
    "    N_images: int,\n",
    "    seed: int,\n",
    "    split: float,\n",
    "    transform: TF.Compose,\n",
    "    batch_size: int = 32,\n",
    "    cache_size: int = None,\n",
    ") -> tuple[idp.IterDataPipe, idp.IterDataPipe]:\n",
    "    IMAGEM_IDX = 1\n",
    "    LABEL_IDX = 0\n",
    "\n",
    "    # link para o arquivo no gdrive\n",
    "    files = idp.FileLister(file, recursive=True, masks=[\"**.jpg\", \"**.png\"])\n",
    "\n",
    "    data_pipe = (\n",
    "        files.open_files(mode=\"rb\")\n",
    "        .read_from_stream()\n",
    "        .map(transform)\n",
    "        .prefetch(N_images)\n",
    "        .shuffle()\n",
    "        .sharding_filter()\n",
    "        .batch(batch_size=batch_size, drop_last=True)\n",
    "        .collate()\n",
    "        .in_memory_cache()\n",
    "    )\n",
    "\n",
    "    split_dict = {\"train\": split, \"valid\": 1 - split}\n",
    "    train, valid = data_pipe.random_split(\n",
    "        weights=split_dict, total_length=N_images, seed=seed\n",
    "    )\n",
    "\n",
    "    return train, valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "\n",
    "Para a arquitetura da rede, decidi por utilizar a `resnet18` por obter boa performance de classificação sem ser ocupar tanto espeço em memória com resnets maiores, além de manter o tempo de inferência mais baixo.\n",
    "\n",
    "Vou realizar uma implementação de fine-tuning da `resnet18` pré treinado fornecida no `Torch Hub` e treinamento do zero da rede para comparar o esforço, performance e tempo de computação necessário para cada implementação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Redes ResNet esperam imagens de entrada com algumas características específicas:\n",
    "\n",
    "1. Tamanho da imagem deve ser no mínimo 224x224\n",
    "2. Pixels com valores entre 0 e 1\n",
    "3. Normalizadas com valoras para RGB para médias e desvio padrão iguais a: `mean = [0.485, 0.456, 0.406]` e `std = [0.229, 0.224, 0.225]`\n",
    "\n",
    "podemos obter essas transformações diretamente de `ResNet18_Weights` ou montá-las manualmente como a seguir:\n",
    "\n",
    "```\n",
    "preproc_func = TF.Compose(\n",
    "[\n",
    "    TF.Resize(256),\n",
    "    TF.CenterCrop(224),\n",
    "    TF.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]\n",
    ")   \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo a transformação fornecida pelo Pytorch por simplicidade. Aqui também crio a pipeline de transformação dos dados e o dataloader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-treinado com fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_vector(label: int, n_classes: int):\n",
    "    # tensor com somente 1 na posicao correspondente ao label\n",
    "    mask = torch.range(1, n_classes) == label\n",
    "\n",
    "    return (torch.ones_like(mask) * mask).type(torch.float)\n",
    "\n",
    "\n",
    "label_to_vector(6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer,\n",
    "    criterion: nn.Module,\n",
    "    data_loader: dl2.DataLoader2,\n",
    "    device: torch.DeviceObjType,\n",
    "    metrics: tmetrics.MetricCollection,\n",
    "    tboard_writer: tboard.SummaryWriter,\n",
    "    n_iter: int,\n",
    ") -> float:\n",
    "    # modelo em modo de treinamento\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    iterator = tqdm(\n",
    "        enumerate(data_loader),\n",
    "        desc=\"train batches\",\n",
    "        unit=\"batches\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for i, (labels, imgs) in iterator:\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # zerando gradientes da ultima atualizacao\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        running_loss += float(loss)\n",
    "\n",
    "        n_iter += i\n",
    "        tboard_writer.add_scalar(\"loss/train/batch/\", float(loss), n_iter)\n",
    "        tboard_writer.add_scalars(\n",
    "            main_tag=\"metrics/train/batch\",\n",
    "            tag_scalar_dict=metrics(\n",
    "                torch.max(torch.softmax(outputs, dim=1), dim=1)[1], labels\n",
    "            ),\n",
    "            global_step=n_iter,\n",
    "        )\n",
    "\n",
    "    tboard_writer.add_scalar(\"loss/train/epoch\", running_loss, n_iter)\n",
    "    tboard_writer.add_scalars(\n",
    "        main_tag=\"metrics/train/epoch\",\n",
    "        tag_scalar_dict=metrics.compute(),\n",
    "        global_step=n_iter,\n",
    "    )\n",
    "\n",
    "    metrics.reset()\n",
    "\n",
    "    return running_loss, n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    data_loader: dl2.DataLoader2,\n",
    "    device: torch.DeviceObjType,\n",
    "    metrics: tmetrics.MetricCollection,\n",
    "    tboard_writer: tboard.SummaryWriter,\n",
    "    n_iter: int,\n",
    ") -> float:\n",
    "    # modelo em modo de validacao\n",
    "    model.eval()\n",
    "\n",
    "    n_claases = model.fc[0].out_features\n",
    "\n",
    "    # equivalente a nograd\n",
    "    running_loss = 0.0\n",
    "    with torch.inference_mode():\n",
    "        iterator = tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"val batches\",\n",
    "            unit=\"batches\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "        )\n",
    "\n",
    "        for i, (labels, imgs) in iterator:\n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            # erro\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += float(loss)\n",
    "\n",
    "            n_iter += i\n",
    "            tboard_writer.add_scalar(\"loss/eval/batch/\", float(loss), n_iter)\n",
    "            tboard_writer.add_scalars(\n",
    "                main_tag=\"metrics/eval/batch\",\n",
    "                tag_scalar_dict=metrics(\n",
    "                    torch.max(torch.softmax(outputs, dim=1), dim=1)[1], labels\n",
    "                ),\n",
    "                global_step=n_iter,\n",
    "            )\n",
    "\n",
    "    tboard_writer.add_scalar(\"loss/eval/epoch\", running_loss, n_iter)\n",
    "    tboard_writer.add_scalars(\n",
    "        main_tag=\"metrics/eval/epoch\",\n",
    "        tag_scalar_dict=metrics.compute(),\n",
    "        global_step=n_iter,\n",
    "    )\n",
    "    metrics.reset()\n",
    "\n",
    "    return running_loss, n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: dl2.DataLoader2,\n",
    "    val_loader: dl2.DataLoader2,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    device: torch.DeviceObjType,\n",
    "    N_epochs: int,\n",
    "    train_metrics: tmetrics.MetricCollection,\n",
    "    val_metrics: tmetrics.MetricCollection,\n",
    "    writer: tboard.SummaryWriter,\n",
    "):\n",
    "    train_loss = eval_loss = lr = 0.0\n",
    "\n",
    "    pbar = trange(\n",
    "        N_epochs,\n",
    "        desc=\"Epocas\",\n",
    "        unit=\"epochs\",\n",
    "        postfix={\"train_loss\": train_loss, \"eval_loss\": eval_loss, \"learning_rate\": lr},\n",
    "        position=0,\n",
    "    )\n",
    "    train_i = val_i = 0\n",
    "    for _ in pbar:\n",
    "        train_loss, train_i = train_epoch(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            device=device,\n",
    "            metrics=train_metrics,\n",
    "            tboard_writer=writer,\n",
    "            n_iter=train_i,\n",
    "        )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        lr = lr_scheduler.get_last_lr()\n",
    "\n",
    "        eval_loss, val_i = eval(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            data_loader=val_loader,\n",
    "            device=device,\n",
    "            metrics=val_metrics,\n",
    "            tboard_writer=writer,\n",
    "            n_iter=val_i,\n",
    "        )\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\"train_loss\": train_loss, \"eval_loss\": eval_loss, \"learning_rate\": lr}\n",
    "        )\n",
    "\n",
    "    train_loader.shutdown()\n",
    "    val_loader.shutdown()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes e hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando os pesos do modelo e a funcao de pre-processamento\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "preproc_transforms = weights.transforms()\n",
    "\n",
    "\n",
    "def combined(example):\n",
    "    return path_to_label(example[0]), preproc_transforms(\n",
    "        image_encoder_to_tensor(example[1])\n",
    "    )\n",
    "\n",
    "\n",
    "# criando pipelines de treinamento e validacao\n",
    "train_pipe, val_pipe = file_pipe(\n",
    "    file=str(DATA_PATH),\n",
    "    N_images=N_IMAGENS,\n",
    "    seed=SEED,\n",
    "    split=SPLIT,\n",
    "    transform=combined,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "reading_service = dl2.MultiProcessingReadingService()\n",
    "train_loader = dl2.DataLoader2(datapipe=train_pipe, reading_service=reading_service)\n",
    "val_loader = dl2.DataLoader2(datapipe=val_pipe, reading_service=reading_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizarmos o fine-tuning, precisamos modificar o ultimo layer `fc` para ter o número de classes do nosso problema como saida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=weights)\n",
    "\n",
    "# mudando tamanho de saida da ultima camada\n",
    "new_input = model.fc.in_features\n",
    "\n",
    "\n",
    "last_layer = nn.Sequential(\n",
    "    nn.Linear(in_features=new_input, out_features=N_CLASSES), nn.Softmax(dim=1)\n",
    ")\n",
    "model.fc = last_layer\n",
    "\n",
    "exp_name = datetime.now().strftime(\"%m:%d-%H:%M\")\n",
    "# writes para vermos estatisticas no tensorboard\n",
    "writer = tboard.SummaryWriter(log_dir=(LOG_PATH / exp_name))\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=optimizer, step_size=LR_STEP, gamma=GAMMA\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(list(freq.negative_logprob / freq.negative_logprob.max()))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(DEVICE)\n",
    "\n",
    "train_metrics = (\n",
    "    tmetrics.MetricCollection(\n",
    "        {\n",
    "            \"acc\": cmetrics.Accuracy(task=\"multiclass\", num_classes=N_CLASSES),\n",
    "            \"fbeta\": cmetrics.FBetaScore(\n",
    "                task=\"multiclass\", beta=F_BETA, num_classes=N_CLASSES\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    .train()\n",
    "    .to(DEVICE)\n",
    ")\n",
    "\n",
    "val_metrics = (\n",
    "    tmetrics.MetricCollection(\n",
    "        {\n",
    "            \"acc\": cmetrics.Accuracy(task=\"multiclass\", num_classes=N_CLASSES),\n",
    "            \"fbeta\": cmetrics.FBetaScore(\n",
    "                task=\"multiclass\", beta=F_BETA, num_classes=N_CLASSES\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    .eval()\n",
    "    .to(DEVICE)\n",
    ")\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=DEVICE,\n",
    "    N_epochs=N_EPOCHS,\n",
    "    train_metrics=train_metrics,\n",
    "    val_metrics=val_metrics,\n",
    "    writer=writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jit nao tem suporte para device \"mps\" do mac, retornamos o modelo para o cpu\n",
    "model.to(torch.device(\"cpu\"))\n",
    "trained_model = torch.jit.script(model.state_dict())\n",
    "trained_model.save(MODELS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_path, test_img):\n",
    "    # carregando modelo\n",
    "    loaded_model = torch.jit.load(\"../models/model.pt\")\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # imagem para teste\n",
    "    # ifuncao de pre-processamento\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    preproc_transforms = weights.transforms()\n",
    "\n",
    "    test_img = preproc_transforms(test_img)\n",
    "\n",
    "    return label_mapping.keys(torch.argmax(loaded_model(test_img)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] <https://doi.org/10.48550/arXiv.1704.04861> \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"\n",
    "\n",
    "[2] <https://doi.org/10.48550/arXiv.1512.03385> \"Deep Residual Learning for Image Recognition\"\n",
    "\n",
    "[3] <https://en.wikipedia.org/wiki/Accuracy_paradox#:~:text=The%20accuracy%20paradox%20is%20the,too%20crude%20to%20be%20useful.> \"Accuracy Paradox\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
